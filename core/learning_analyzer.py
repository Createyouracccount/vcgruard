"""
VoiceGuard AI - í•™ìŠµ ê°•í™” ë³´ì´ìŠ¤í”¼ì‹± ë¶„ì„ê¸° (ìˆœí™˜ import í•´ê²° ë²„ì „)
ê¸°ì¡´ ì‹œìŠ¤í…œì— ì ì§„ì  í•™ìŠµ ê¸°ëŠ¥ ì¶”ê°€

íŒŒì¼ ìœ„ì¹˜: core/learning_enhanced_analyzer.py
"""

import asyncio
import logging
import json
import time
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
import hashlib

from collections import defaultdict, Counter

# ìˆœí™˜ import ë°©ì§€ - analyzerëŠ” ë™ì  ë¡œë”©
from core.llm_manager import llm_manager
from config.settings import detection_thresholds

logger = logging.getLogger(__name__)

@dataclass
class LearningExample:
    """í•™ìŠµ ì˜ˆì‹œ ë°ì´í„°"""
    text: str
    actual_label: str  # "scam" or "legitimate"
    predicted_label: str
    confidence: float
    user_feedback: str  # "correct", "wrong", "uncertain"
    cultural_markers: List[str]
    timestamp: datetime
    user_id: Optional[str] = None

@dataclass
class AdaptivePattern:
    """ì ì‘í˜• íŒ¨í„´"""
    pattern_id: str
    keywords: List[str]
    cultural_context: List[str]
    success_rate: float
    usage_count: int
    last_updated: datetime
    examples: List[str]

class LearningEnhancedAnalyzer:
    """í•™ìŠµ ê¸°ëŠ¥ì´ ê°•í™”ëœ ë¶„ì„ê¸°"""
    
    def __init__(self, llm_manager):
        self.llm_manager = llm_manager
        
        # ê¸°ë³¸ ë¶„ì„ ê¸°ëŠ¥ (analyzer.pyì—ì„œ ë³µì‚¬)
        self.stats = {
            'total_analyses': 0,
            'high_risk_detections': 0,
            'avg_analysis_time': 0.0,
            'pattern_matches': {}
        }
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ íŒ¨í„´
        self.quick_patterns = self._build_quick_patterns()
        
        # í•™ìŠµ ë°ì´í„° ì €ìž¥ì†Œ
        self.learning_examples: List[LearningExample] = []
        self.adaptive_patterns: Dict[str, AdaptivePattern] = {}
        
        # Few-shot ì˜ˆì‹œ í’€
        self.few_shot_pool = self._initialize_few_shot_pool()
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_tracker = {
            "total_analyses": 0,
            "user_feedback_count": 0,
            "accuracy_trend": [],
            "pattern_evolution": [],
            "learning_cycles": 0
        }
        
        # ì„¤ì •
        self.config = {
            "min_examples_for_pattern": 3,
            "max_few_shot_examples": 5,
            "learning_threshold": 10,  # 10ê°œ í”¼ë“œë°±ë§ˆë‹¤ í•™ìŠµ
            "pattern_confidence_threshold": 0.7,
            "data_persistence_path": Path("data/learning_data.json")
        }
        
        # ê¸°ì¡´ í•™ìŠµ ë°ì´í„° ë¡œë“œ
        self._load_existing_data()
        
        logger.info("ðŸ§  í•™ìŠµ ê°•í™” ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _build_quick_patterns(self) -> Dict[str, Any]:
        """ê¸°ë³¸ í‚¤ì›Œë“œ íŒ¨í„´ (analyzer.pyì™€ ë™ì¼)"""
        
        patterns = {
            'critical_keywords': [
                'ë‚©ì¹˜', 'ìœ ê´´', 'ì£½ëŠ”ë‹¤', 'ì²´í¬ì˜ìž¥', 'ê³„ì¢Œë™ê²°', 'ì‘ê¸‰ì‹¤'
            ],
            'high_risk_keywords': [
                'ê¸ˆìœµê°ë…ì›', 'ê²€ì°°ì²­', 'ê²½ì°°ì„œ', 'ìˆ˜ì‚¬', 'ì¡°ì‚¬', 'ë²”ì£„', 'í”¼ì˜ìž'
            ],
            'medium_risk_keywords': [
                'ëŒ€ì¶œ', 'ì €ê¸ˆë¦¬', 'ì •ë¶€ì§€ì›ê¸ˆ', 'í™˜ê¸‰', 'ë‹¹ì²¨', 'ë§Œë‚˜ì„œ', 'ì§ì ‘'
            ],
            'financial_keywords': [
                'ê³„ì¢Œë²ˆí˜¸', 'ë¹„ë°€ë²ˆí˜¸', 'ì†¡ê¸ˆ', 'ì´ì²´', 'í˜„ê¸ˆ', 'ì¹´ë“œë²ˆí˜¸'
            ],
            'app_keywords': [
                'ì•±ì„¤ì¹˜', 'ë‹¤ìš´ë¡œë“œ', 'ê¶Œí•œ', 'í—ˆìš©', 'ì—…ë°ì´íŠ¸', 'ì¸ì¦'
            ]
        }
        
        return patterns
    
    def _initialize_few_shot_pool(self) -> Dict[str, List[Dict]]:
        """Few-shot ì˜ˆì‹œ í’€ ì´ˆê¸°í™”"""
        
        return {
            "government_scam": [
                {
                    "text": "ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ. ê¸ˆìœµê°ë…ì› ì „ìžê¸ˆìœµê³¼ì—ì„œ ì—°ë½ë“œë ¸ìŠµë‹ˆë‹¤. ê³ ê°ë‹˜ì˜ ê³„ì¢Œê°€ ë²”ì£„ì— ì—°ë£¨ë˜ì–´ ì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                    "label": "scam",
                    "reasoning": "ê¸ˆìœµê°ë…ì›ì€ ê°œì¸ì—ê²Œ ì§ì ‘ ì „í™”í•˜ì—¬ ê³„ì¢Œ ê´€ë ¨ ì¡°ì¹˜ë¥¼ ìš”êµ¬í•˜ì§€ ì•ŠìŒ",
                    "confidence": 0.95
                },
                {
                    "text": "ê²€ì°°ì²­ ìˆ˜ì‚¬ê´€ìž…ë‹ˆë‹¤. ê·€í•˜ì™€ ê´€ë ¨ëœ ì‚¬ê±´ìœ¼ë¡œ ì¶œì„í•˜ì…”ì•¼ í•©ë‹ˆë‹¤.",
                    "label": "scam", 
                    "reasoning": "ê²€ì°°ì²­ ì¶œì„ì€ ê³µë¬¸ìœ¼ë¡œ í†µì§€í•˜ë©° ì „í™”ë¡œ ê°œì¸ ì—°ë½í•˜ì§€ ì•ŠìŒ",
                    "confidence": 0.93
                }
            ],
            
            "family_emergency": [
                {
                    "text": "ì–´ë¨¸ë‹ˆ, ì € ì•„ë“¤ì´ì—ìš”. êµí†µì‚¬ê³  ë‚˜ì„œ ì‘ê¸‰ì‹¤ì— ìžˆëŠ”ë° ìˆ˜ìˆ ë¹„ê°€ ê¸‰í•´ìš”.",
                    "label": "scam",
                    "reasoning": "ê°€ì¡± í™•ì¸ ì—†ì´ ê¸‰í•œ ê¸ˆì „ ìš”êµ¬, ì‹¤ì œ ì‘ê¸‰ìƒí™©ì‹œ ë³‘ì›ì—ì„œ ê³µì‹ ì—°ë½",
                    "confidence": 0.88
                },
                {
                    "text": "í• ì•„ë²„ì§€, ì†ìžì˜ˆìš”. ì‚¬ì±„ì—…ìžë“¤í•œí…Œ ìž¡í˜€ì„œ ëˆì´ ê¸‰í•´ìš”.",
                    "label": "scam",
                    "reasoning": "ê°€ì¡± ê´€ê³„ ì´ìš©í•œ ê°ì • ì¡°ìž‘, ë¶ˆë²• ìƒí™© ì–¸ê¸‰ìœ¼ë¡œ ì‹ ê³  íšŒí”¼ ìœ ë„",
                    "confidence": 0.91
                }
            ],
            
            "legitimate_calls": [
                {
                    "text": "ì•ˆë…•í•˜ì„¸ìš”. ê³ ê°ë‹˜ê»˜ì„œ ì‹ ì²­í•˜ì‹  ëŒ€ì¶œ ì‹¬ì‚¬ ê²°ê³¼ë¥¼ ì•ˆë‚´ë“œë¦¬ë ¤ê³  ì—°ë½ë“œë ¸ìŠµë‹ˆë‹¤.",
                    "label": "legitimate",
                    "reasoning": "ê³ ê°ì´ ì‚¬ì „ ì‹ ì²­í•œ ì„œë¹„ìŠ¤ì— ëŒ€í•œ ê²°ê³¼ ì•ˆë‚´",
                    "confidence": 0.85
                },
                {
                    "text": "ë°°ì†¡ ì˜ˆì •ì´ë˜ íƒë°°ê°€ ì£¼ì†Œ ë¬¸ì œë¡œ ë°°ì†¡ì´ ì–´ë ¤ì›Œ ì—°ë½ë“œë ¸ìŠµë‹ˆë‹¤.",
                    "label": "legitimate",
                    "reasoning": "êµ¬ì²´ì ì¸ ë°°ì†¡ ê´€ë ¨ ë¬¸ì˜, ê°œì¸ì •ë³´ ìš”êµ¬ ì—†ìŒ",
                    "confidence": 0.80
                }
            ]
        }
    
    async def analyze_text(self, text: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """ê¸°ë³¸ analyze_text ë©”ì„œë“œ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        
        # í•™ìŠµ ê°•í™” ë¶„ì„ ìˆ˜í–‰
        enhanced_result = await self.analyze_with_learning(text, context)
        
        # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)
        legacy_result = {
            "risk_score": enhanced_result["final_risk_score"],
            "risk_level": enhanced_result["risk_level"],
            "scam_type": enhanced_result["scam_type"],
            "confidence": enhanced_result["confidence"],
            "key_indicators": enhanced_result["key_indicators"],
            "immediate_action": enhanced_result["final_risk_score"] >= 0.8,
            "reasoning": enhanced_result["reasoning"],
            "recommendation": enhanced_result["recommendation"],
            
            # ìƒˆë¡œìš´ í•™ìŠµ ì •ë³´ ì¶”ê°€
            "learning_enhanced": True,
            "analysis_id": enhanced_result["analysis_id"],
            "few_shot_applied": enhanced_result["few_shot_applied"],
            "patterns_matched": enhanced_result["patterns_matched"]
        }
        
        return legacy_result
    
    async def analyze_with_learning(self, text: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """í•™ìŠµ ê¸°ëŠ¥ì´ í†µí•©ëœ ë¶„ì„"""
        
        start_time = time.time()
        context = context or {}
        
        # 1. ê¸°ë³¸ ë¶„ì„ ìˆ˜í–‰ (í‚¤ì›Œë“œ + LLM)
        base_result = await self._basic_analysis(text, context)
        
        # 2. Few-shot Learning ì ìš©
        few_shot_result = await self._apply_few_shot_learning(text, context)
        
        # 3. ì ì‘í˜• íŒ¨í„´ ë§¤ì¹­
        pattern_result = self._apply_adaptive_patterns(text)
        
        # 4. ê²°ê³¼ í†µí•© ë° ì‹ ë¢°ë„ ì¡°ì •
        integrated_result = self._integrate_analysis_results(
            base_result, few_shot_result, pattern_result
        )
        
        # 5. í•™ìŠµì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        integrated_result.update({
            "analysis_id": self._generate_analysis_id(text),
            "few_shot_applied": few_shot_result.get("examples_used", 0) > 0,
            "patterns_matched": len(pattern_result.get("matched_patterns", [])),
            "learning_data": {
                "base_confidence": base_result.get("confidence", 0),
                "few_shot_confidence": few_shot_result.get("confidence", 0),
                "pattern_confidence": pattern_result.get("confidence", 0)
            }
        })