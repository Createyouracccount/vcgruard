"""
VoiceGuard AI - í•™ìŠµ ê°•í™” ë³´ì´ìŠ¤í”¼ì‹± ë¶„ì„ê¸°
ê¸°ì¡´ ì‹œìŠ¤í…œì— ì ì§„ì  í•™ìŠµ ê¸°ëŠ¥ ì¶”ê°€
"""

import asyncio
import logging
import json
import time
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
import hashlib

from collections import defaultdict, Counter

# ìˆœí™˜ import ë°©ì§€ - analyzerëŠ” ë™ì  ë¡œë”©
from core.llm_manager import llm_manager
from config.settings import detection_thresholds

logger = logging.getLogger(__name__)

logger = logging.getLogger(__name__)

@dataclass
class LearningExample:
    """í•™ìŠµ ì˜ˆì‹œ ë°ì´í„°"""
    text: str
    actual_label: str  # "scam" or "legitimate"
    predicted_label: str
    confidence: float
    user_feedback: str  # "correct", "wrong", "uncertain"
    cultural_markers: List[str]
    timestamp: datetime
    user_id: Optional[str] = None

@dataclass
class AdaptivePattern:
    """ì ì‘í˜• íŒ¨í„´"""
    pattern_id: str
    keywords: List[str]
    cultural_context: List[str]
    success_rate: float
    usage_count: int
    last_updated: datetime
    examples: List[str]

class LearningEnhancedAnalyzer:
    """í•™ìŠµ ê¸°ëŠ¥ì´ ê°•í™”ëœ ë¶„ì„ê¸°"""
    
    def __init__(self, llm_manager):
        self.llm_manager = llm_manager
        
        # ê¸°ë³¸ ë¶„ì„ ê¸°ëŠ¥ (analyzer.pyì—ì„œ ë³µì‚¬)
        self.stats = {
            'total_analyses': 0,
            'high_risk_detections': 0,
            'avg_analysis_time': 0.0,
            'pattern_matches': {}
        }
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ íŒ¨í„´
        self.quick_patterns = self._build_quick_patterns()
        
        # í•™ìŠµ ë°ì´í„° ì €ì¥ì†Œ
        self.learning_examples: List[LearningExample] = []
        self.adaptive_patterns: Dict[str, AdaptivePattern] = {}
        
        # Few-shot ì˜ˆì‹œ í’€
        self.few_shot_pool = self._initialize_few_shot_pool()
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_tracker = {
            "total_analyses": 0,
            "user_feedback_count": 0,
            "accuracy_trend": [],
            "pattern_evolution": [],
            "learning_cycles": 0
        }
        
        # ì„¤ì •
        self.config = {
            "min_examples_for_pattern": 3,
            "max_few_shot_examples": 5,
            "learning_threshold": 10,  # 10ê°œ í”¼ë“œë°±ë§ˆë‹¤ í•™ìŠµ
            "pattern_confidence_threshold": 0.7,
            "data_persistence_path": Path("data/learning_data.json")
        }
        
        # ê¸°ì¡´ í•™ìŠµ ë°ì´í„° ë¡œë“œ
        self._load_existing_data()
        
        logger.info("ğŸ§  í•™ìŠµ ê°•í™” ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _build_quick_patterns(self) -> Dict[str, Any]:
        """ê¸°ë³¸ í‚¤ì›Œë“œ íŒ¨í„´ (analyzer.pyì™€ ë™ì¼)"""
        
        patterns = {
            'critical_keywords': [
                'ë‚©ì¹˜', 'ìœ ê´´', 'ì£½ëŠ”ë‹¤', 'ì²´í¬ì˜ì¥', 'ê³„ì¢Œë™ê²°', 'ì‘ê¸‰ì‹¤'
            ],
            'high_risk_keywords': [
                'ê¸ˆìœµê°ë…ì›', 'ê²€ì°°ì²­', 'ê²½ì°°ì„œ', 'ìˆ˜ì‚¬', 'ì¡°ì‚¬', 'ë²”ì£„', 'í”¼ì˜ì'
            ],
            'medium_risk_keywords': [
                'ëŒ€ì¶œ', 'ì €ê¸ˆë¦¬', 'ì •ë¶€ì§€ì›ê¸ˆ', 'í™˜ê¸‰', 'ë‹¹ì²¨', 'ë§Œë‚˜ì„œ', 'ì§ì ‘'
            ],
            'financial_keywords': [
                'ê³„ì¢Œë²ˆí˜¸', 'ë¹„ë°€ë²ˆí˜¸', 'ì†¡ê¸ˆ', 'ì´ì²´', 'í˜„ê¸ˆ', 'ì¹´ë“œë²ˆí˜¸'
            ],
            'app_keywords': [
                'ì•±ì„¤ì¹˜', 'ë‹¤ìš´ë¡œë“œ', 'ê¶Œí•œ', 'í—ˆìš©', 'ì—…ë°ì´íŠ¸', 'ì¸ì¦'
            ]
        }
        
        return patterns
    
    def _initialize_few_shot_pool(self) -> Dict[str, List[Dict]]:
        """Few-shot ì˜ˆì‹œ í’€ ì´ˆê¸°í™”"""
        
        return {
            "government_scam": [
                {
                    "text": "ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ. ê¸ˆìœµê°ë…ì› ì „ìê¸ˆìœµê³¼ì—ì„œ ì—°ë½ë“œë ¸ìŠµë‹ˆë‹¤. ê³ ê°ë‹˜ì˜ ê³„ì¢Œê°€ ë²”ì£„ì— ì—°ë£¨ë˜ì–´ ì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                    "label": "scam",
                    "reasoning": "ê¸ˆìœµê°ë…ì›ì€ ê°œì¸ì—ê²Œ ì§ì ‘ ì „í™”í•˜ì—¬ ê³„ì¢Œ ê´€ë ¨ ì¡°ì¹˜ë¥¼ ìš”êµ¬í•˜ì§€ ì•ŠìŒ",
                    "confidence": 0.95
                },
                {
                    "text": "ê²€ì°°ì²­ ìˆ˜ì‚¬ê´€ì…ë‹ˆë‹¤. ê·€í•˜ì™€ ê´€ë ¨ëœ ì‚¬ê±´ìœ¼ë¡œ ì¶œì„í•˜ì…”ì•¼ í•©ë‹ˆë‹¤.",
                    "label": "scam", 
                    "reasoning": "ê²€ì°°ì²­ ì¶œì„ì€ ê³µë¬¸ìœ¼ë¡œ í†µì§€í•˜ë©° ì „í™”ë¡œ ê°œì¸ ì—°ë½í•˜ì§€ ì•ŠìŒ",
                    "confidence": 0.93
                }
            ],
            
            "family_emergency": [
                {
                    "text": "ì–´ë¨¸ë‹ˆ, ì € ì•„ë“¤ì´ì—ìš”. êµí†µì‚¬ê³  ë‚˜ì„œ ì‘ê¸‰ì‹¤ì— ìˆëŠ”ë° ìˆ˜ìˆ ë¹„ê°€ ê¸‰í•´ìš”.",
                    "label": "scam",
                    "reasoning": "ê°€ì¡± í™•ì¸ ì—†ì´ ê¸‰í•œ ê¸ˆì „ ìš”êµ¬, ì‹¤ì œ ì‘ê¸‰ìƒí™©ì‹œ ë³‘ì›ì—ì„œ ê³µì‹ ì—°ë½",
                    "confidence": 0.88
                },
                {
                    "text": "í• ì•„ë²„ì§€, ì†ìì˜ˆìš”. ì‚¬ì±„ì—…ìë“¤í•œí…Œ ì¡í˜€ì„œ ëˆì´ ê¸‰í•´ìš”.",
                    "label": "scam",
                    "reasoning": "ê°€ì¡± ê´€ê³„ ì´ìš©í•œ ê°ì • ì¡°ì‘, ë¶ˆë²• ìƒí™© ì–¸ê¸‰ìœ¼ë¡œ ì‹ ê³  íšŒí”¼ ìœ ë„",
                    "confidence": 0.91
                }
            ],
            
            "legitimate_calls": [
                {
                    "text": "ì•ˆë…•í•˜ì„¸ìš”. ê³ ê°ë‹˜ê»˜ì„œ ì‹ ì²­í•˜ì‹  ëŒ€ì¶œ ì‹¬ì‚¬ ê²°ê³¼ë¥¼ ì•ˆë‚´ë“œë¦¬ë ¤ê³  ì—°ë½ë“œë ¸ìŠµë‹ˆë‹¤.",
                    "label": "legitimate",
                    "reasoning": "ê³ ê°ì´ ì‚¬ì „ ì‹ ì²­í•œ ì„œë¹„ìŠ¤ì— ëŒ€í•œ ê²°ê³¼ ì•ˆë‚´",
                    "confidence": 0.85
                },
                {
                    "text": "ë°°ì†¡ ì˜ˆì •ì´ë˜ íƒë°°ê°€ ì£¼ì†Œ ë¬¸ì œë¡œ ë°°ì†¡ì´ ì–´ë ¤ì›Œ ì—°ë½ë“œë ¸ìŠµë‹ˆë‹¤.",
                    "label": "legitimate",
                    "reasoning": "êµ¬ì²´ì ì¸ ë°°ì†¡ ê´€ë ¨ ë¬¸ì˜, ê°œì¸ì •ë³´ ìš”êµ¬ ì—†ìŒ",
                    "confidence": 0.80
                }
            ]
        }
    
    async def analyze_text(self, text: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """ê¸°ë³¸ analyze_text ë©”ì„œë“œ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        
        # í•™ìŠµ ê°•í™” ë¶„ì„ ìˆ˜í–‰
        enhanced_result = await self.analyze_with_learning(text, context)
        
        # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)
        legacy_result = {
            "risk_score": enhanced_result["final_risk_score"],
            "risk_level": enhanced_result["risk_level"],
            "scam_type": enhanced_result["scam_type"],
            "confidence": enhanced_result["confidence"],
            "key_indicators": enhanced_result["key_indicators"],
            "immediate_action": enhanced_result["final_risk_score"] >= 0.8,
            "reasoning": enhanced_result["reasoning"],
            "recommendation": enhanced_result["recommendation"],
            
            # ìƒˆë¡œìš´ í•™ìŠµ ì •ë³´ ì¶”ê°€
            "learning_enhanced": True,
            "analysis_id": enhanced_result["analysis_id"],
            "few_shot_applied": enhanced_result["few_shot_applied"],
            "patterns_matched": enhanced_result["patterns_matched"]
        }
        
        return legacy_result
    
    async def analyze_with_learning(self, text: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """í•™ìŠµ ê¸°ëŠ¥ì´ í†µí•©ëœ ë¶„ì„"""
        
        start_time = time.time()
        context = context or {}
        
        # 1. ê¸°ë³¸ ë¶„ì„ ìˆ˜í–‰
        base_result = await super().analyze_text(text, context)
        
        # 2. Few-shot Learning ì ìš©
        few_shot_result = await self._apply_few_shot_learning(text, context)
        
        # 3. ì ì‘í˜• íŒ¨í„´ ë§¤ì¹­
        pattern_result = self._apply_adaptive_patterns(text)
        
        # 4. ê²°ê³¼ í†µí•© ë° ì‹ ë¢°ë„ ì¡°ì •
        integrated_result = self._integrate_analysis_results(
            base_result, few_shot_result, pattern_result
        )
        
        # 5. í•™ìŠµì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        integrated_result.update({
            "analysis_id": self._generate_analysis_id(text),
            "few_shot_applied": few_shot_result.get("examples_used", 0) > 0,
            "patterns_matched": len(pattern_result.get("matched_patterns", [])),
            "learning_data": {
                "base_confidence": base_result.get("confidence", 0),
                "few_shot_confidence": few_shot_result.get("confidence", 0),
                "pattern_confidence": pattern_result.get("confidence", 0)
            }
        })
        
        # 6. ì„±ëŠ¥ ì¶”ì  ì—…ë°ì´íŠ¸
        self.performance_tracker["total_analyses"] += 1
        
        processing_time = time.time() - start_time
        logger.info(f"ğŸ§  í•™ìŠµ ê°•í™” ë¶„ì„ ì™„ë£Œ: {processing_time:.3f}ì´ˆ")
        
        return integrated_result
    
    async def _apply_few_shot_learning(self, text: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Few-shot Learning ì ìš©"""
        
        # ê´€ë ¨ì„± ë†’ì€ ì˜ˆì‹œ ì„ ë³„
        relevant_examples = self._select_relevant_examples(text)
        
        if not relevant_examples:
            return {"confidence": 0, "examples_used": 0}
        
        # Few-shot í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_few_shot_prompt(text, relevant_examples)
        
        try:
            # LLM í˜¸ì¶œ
            result = await self.llm_manager.analyze_scam_risk(
                text=prompt,
                context={**context, "analysis_type": "few_shot_learning"}
            )
            
            # ê²°ê³¼ íŒŒì‹±
            parsed_result = self._parse_few_shot_response(result.content)
            parsed_result["examples_used"] = len(relevant_examples)
            
            return parsed_result
            
        except Exception as e:
            logger.error(f"Few-shot í•™ìŠµ ì‹¤íŒ¨: {e}")
            return {"confidence": 0, "examples_used": 0, "error": str(e)}
    
    def _select_relevant_examples(self, text: str, max_examples: int = None) -> List[Dict]:
        """ê´€ë ¨ì„± ë†’ì€ ì˜ˆì‹œ ì„ ë³„"""
        
        max_examples = max_examples or self.config["max_few_shot_examples"]
        relevant_examples = []
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ì„± ê³„ì‚°
        text_words = set(text.lower().split())
        
        for category, examples in self.few_shot_pool.items():
            for example in examples:
                example_words = set(example["text"].lower().split())
                
                # ê³µí†µ ë‹¨ì–´ ë¹„ìœ¨ ê³„ì‚°
                common_words = text_words & example_words
                relevance_score = len(common_words) / max(len(text_words), len(example_words))
                
                if relevance_score > 0.1:  # ìµœì†Œ ê´€ë ¨ì„± ì„ê³„ê°’
                    example_with_score = example.copy()
                    example_with_score["relevance_score"] = relevance_score
                    relevant_examples.append(example_with_score)
        
        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ Nê°œ ì„ íƒ
        relevant_examples.sort(key=lambda x: x["relevance_score"], reverse=True)
        return relevant_examples[:max_examples]
    
    def _build_few_shot_prompt(self, text: str, examples: List[Dict]) -> str:
        """Few-shot í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        prompt = """ë‹¹ì‹ ì€ í•œêµ­ ë³´ì´ìŠ¤í”¼ì‹± íƒì§€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ í•™ìŠµëœ ì˜ˆì‹œë“¤ì„ ì°¸ê³ í•˜ì—¬ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì„¸ìš”.

## í•™ìŠµ ì˜ˆì‹œë“¤:
"""
        
        for i, example in enumerate(examples, 1):
            prompt += f"""
ì˜ˆì‹œ {i}:
í…ìŠ¤íŠ¸: "{example['text']}"
íŒì •: {example['label']}
ì‹ ë¢°ë„: {example['confidence']:.2f}
ê·¼ê±°: {example['reasoning']}
---
"""
        
        prompt += f"""

## ë¶„ì„í•  í…ìŠ¤íŠ¸:
"{text}"

## ì§€ì‹œì‚¬í•­:
ìœ„ í•™ìŠµ ì˜ˆì‹œë“¤ì˜ íŒ¨í„´ì„ ì°¸ê³ í•˜ì—¬ ë¶„ì„í•˜ê³ , ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "risk_score": 0.0-1.0,
    "classification": "scam" ë˜ëŠ” "legitimate",
    "confidence": 0.0-1.0,
    "reasoning": "íŒë‹¨ ê·¼ê±° (í•™ìŠµ ì˜ˆì‹œì™€ì˜ ìœ ì‚¬ì  ì–¸ê¸‰)",
    "similar_examples": ["ìœ ì‚¬í•œ ì˜ˆì‹œ ë²ˆí˜¸ë“¤"],
    "key_patterns": ["íƒì§€ëœ ì£¼ìš” íŒ¨í„´ë“¤"]
}}
"""
        
        return prompt
    
    def _parse_few_shot_response(self, response: str) -> Dict[str, Any]:
        """Few-shot ì‘ë‹µ íŒŒì‹±"""
        
        try:
            # JSON íŒŒì‹± ì‹œë„
            import json
            parsed = json.loads(response)
            
            return {
                "confidence": parsed.get("confidence", 0.5),
                "risk_score": parsed.get("risk_score", 0.5),
                "classification": parsed.get("classification", "uncertain"),
                "reasoning": parsed.get("reasoning", ""),
                "similar_examples": parsed.get("similar_examples", []),
                "key_patterns": parsed.get("key_patterns", [])
            }
            
        except json.JSONDecodeError:
            # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
            logger.warning("Few-shot ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ë¶„ì„ ì‹œë„")
            
            confidence = 0.5
            risk_score = 0.5
            
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¶„ì„
            if "scam" in response.lower():
                classification = "scam"
                risk_score = 0.7
            elif "legitimate" in response.lower():
                classification = "legitimate"
                risk_score = 0.3
            else:
                classification = "uncertain"
            
            return {
                "confidence": confidence,
                "risk_score": risk_score,
                "classification": classification,
                "reasoning": "ì‘ë‹µ íŒŒì‹± ë¶€ë¶„ ì‹¤íŒ¨",
                "similar_examples": [],
                "key_patterns": []
            }
    
    def _apply_adaptive_patterns(self, text: str) -> Dict[str, Any]:
        """ì ì‘í˜• íŒ¨í„´ ë§¤ì¹­"""
        
        matched_patterns = []
        total_confidence = 0
        
        for pattern_id, pattern in self.adaptive_patterns.items():
            # í‚¤ì›Œë“œ ë§¤ì¹­
            keyword_matches = sum(1 for keyword in pattern.keywords if keyword in text.lower())
            
            if keyword_matches > 0:
                match_score = (keyword_matches / len(pattern.keywords)) * pattern.success_rate
                
                matched_patterns.append({
                    "pattern_id": pattern_id,
                    "match_score": match_score,
                    "keywords_matched": keyword_matches,
                    "pattern_success_rate": pattern.success_rate,
                    "usage_count": pattern.usage_count
                })
                
                total_confidence += match_score
        
        # ìµœê³  ë§¤ì¹­ íŒ¨í„´
        best_pattern = max(matched_patterns, key=lambda x: x["match_score"]) if matched_patterns else None
        
        return {
            "confidence": min(total_confidence, 1.0),
            "matched_patterns": matched_patterns,
            "best_pattern": best_pattern,
            "total_patterns_checked": len(self.adaptive_patterns)
        }
    
    def _integrate_analysis_results(self, base_result: Dict, few_shot_result: Dict, pattern_result: Dict) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ í†µí•©"""
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ìœ„í—˜ë„ ê³„ì‚°
        weights = {
            "base": 0.4,
            "few_shot": 0.4, 
            "pattern": 0.2
        }
        
        # ê° ë¶„ì„ì˜ ìœ„í—˜ë„ ì ìˆ˜
        base_score = base_result.get("risk_score", 0.5)
        few_shot_score = few_shot_result.get("risk_score", 0.5)
        pattern_score = pattern_result.get("confidence", 0.5)
        
        final_risk_score = (
            base_score * weights["base"] +
            few_shot_score * weights["few_shot"] +
            pattern_score * weights["pattern"]
        )
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidences = [
            base_result.get("confidence", 0.5),
            few_shot_result.get("confidence", 0.5),
            pattern_result.get("confidence", 0.5)
        ]
        
        final_confidence = sum(confidences) / len(confidences)
        
        # ìœ„í—˜ ë ˆë²¨ ê²°ì •
        if final_risk_score >= 0.8:
            risk_level = "critical"
        elif final_risk_score >= 0.6:
            risk_level = "high"
        elif final_risk_score >= 0.4:
            risk_level = "medium"
        else:
            risk_level = "low"
        
        # í†µí•© ê²°ê³¼
        return {
            "final_risk_score": final_risk_score,
            "risk_level": risk_level,
            "confidence": final_confidence,
            "scam_type": base_result.get("scam_type", "unknown"),
            "key_indicators": base_result.get("key_indicators", []),
            "reasoning": self._create_integrated_reasoning(base_result, few_shot_result, pattern_result),
            "recommendation": base_result.get("recommendation", "ì‹ ì¤‘í•˜ê²Œ ëŒ€ì‘í•˜ì„¸ìš”"),
            "analysis_breakdown": {
                "base_analysis": {"score": base_score, "confidence": base_result.get("confidence", 0.5)},
                "few_shot_learning": {"score": few_shot_score, "confidence": few_shot_result.get("confidence", 0.5)},
                "pattern_matching": {"score": pattern_score, "confidence": pattern_result.get("confidence", 0.5)}
            }
        }
    
    def _create_integrated_reasoning(self, base_result: Dict, few_shot_result: Dict, pattern_result: Dict) -> str:
        """í†µí•© ì¶”ë¡  ê²°ê³¼ ìƒì„±"""
        
        reasoning_parts = []
        
        # ê¸°ë³¸ ë¶„ì„ ì¶”ë¡ 
        if base_result.get("reasoning"):
            reasoning_parts.append(f"ê¸°ë³¸ ë¶„ì„: {base_result['reasoning']}")
        
        # Few-shot í•™ìŠµ ì¶”ë¡ 
        if few_shot_result.get("reasoning") and few_shot_result.get("examples_used", 0) > 0:
            reasoning_parts.append(f"í•™ìŠµ ê¸°ë°˜ ë¶„ì„: {few_shot_result['reasoning']}")
        
        # íŒ¨í„´ ë§¤ì¹­ ê²°ê³¼
        if pattern_result.get("best_pattern"):
            best_pattern = pattern_result["best_pattern"]
            reasoning_parts.append(f"íŒ¨í„´ ë§¤ì¹­: {best_pattern['pattern_id']} íŒ¨í„´ê³¼ {best_pattern['match_score']:.2f} ì¼ì¹˜")
        
        return " | ".join(reasoning_parts) if reasoning_parts else "ì¢…í•© ë¶„ì„ ì™„ë£Œ"
    
    async def learn_from_feedback(self, analysis_id: str, actual_label: str, user_feedback: str, user_id: str = None):
        """ì‚¬ìš©ì í”¼ë“œë°±ìœ¼ë¡œë¶€í„° í•™ìŠµ"""
        
        # ë¶„ì„ ê²°ê³¼ ì°¾ê¸° (ì‹¤ì œë¡œëŠ” ì„¸ì…˜ ì €ì¥ì†Œì—ì„œ ì¡°íšŒ)
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í™”í•˜ì—¬ ìƒˆë¡œìš´ í•™ìŠµ ì˜ˆì‹œë¡œ ì²˜ë¦¬
        
        learning_example = LearningExample(
            text="", # ì‹¤ì œë¡œëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ ì €ì¥
            actual_label=actual_label,
            predicted_label="", # ì‹¤ì œë¡œëŠ” ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            confidence=0.0, # ì‹¤ì œë¡œëŠ” ì˜ˆì¸¡ ì‹ ë¢°ë„ ì €ì¥
            user_feedback=user_feedback,
            cultural_markers=[],
            timestamp=datetime.now(),
            user_id=user_id
        )
        
        self.learning_examples.append(learning_example)
        self.performance_tracker["user_feedback_count"] += 1
        
        logger.info(f"ğŸ“ ì‚¬ìš©ì í”¼ë“œë°± í•™ìŠµ: {user_feedback} (ì´ {len(self.learning_examples)}ê°œ)")
        
        # ì¶©ë¶„í•œ í”¼ë“œë°±ì´ ìŒ“ì´ë©´ í•™ìŠµ ìˆ˜í–‰
        if len(self.learning_examples) >= self.config["learning_threshold"]:
            await self._perform_learning_cycle()
    
    async def _perform_learning_cycle(self):
        """í•™ìŠµ ì‚¬ì´í´ ìˆ˜í–‰"""
        
        logger.info("ğŸ”„ í•™ìŠµ ì‚¬ì´í´ ì‹œì‘...")
        
        try:
            # 1. ì˜ëª» ë¶„ë¥˜ëœ ì‚¬ë¡€ë“¤ ë¶„ì„
            incorrect_examples = [
                ex for ex in self.learning_examples 
                if ex.user_feedback == "wrong"
            ]
            
            # 2. ìƒˆë¡œìš´ íŒ¨í„´ ë°œê²¬
            await self._discover_new_patterns(incorrect_examples)
            
            # 3. Few-shot ì˜ˆì‹œ í’€ ì—…ë°ì´íŠ¸
            self._update_few_shot_pool()
            
            # 4. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            self._calculate_performance_metrics()
            
            # 5. í•™ìŠµ ë°ì´í„° ì €ì¥
            self._save_learning_data()
            
            self.performance_tracker["learning_cycles"] += 1
            
            logger.info(f"âœ… í•™ìŠµ ì‚¬ì´í´ ì™„ë£Œ (#{self.performance_tracker['learning_cycles']})")
            
        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì‚¬ì´í´ ì‹¤íŒ¨: {e}")
    
    async def _discover_new_patterns(self, incorrect_examples: List[LearningExample]):
        """ìƒˆë¡œìš´ íŒ¨í„´ ë°œê²¬"""
        
        if len(incorrect_examples) < self.config["min_examples_for_pattern"]:
            return
        
        # ê³µí†µ í‚¤ì›Œë“œ ì¶”ì¶œ
        scam_examples = [ex for ex in incorrect_examples if ex.actual_label == "scam"]
        
        if len(scam_examples) >= 3:
            # í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„
            all_words = []
            for example in scam_examples:
                all_words.extend(example.text.lower().split())
            
            from collections import Counter
            word_freq = Counter(all_words)
            
            # ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” í‚¤ì›Œë“œë“¤ë¡œ ìƒˆ íŒ¨í„´ ìƒì„±
            common_keywords = [word for word, freq in word_freq.most_common(10) if freq >= 2]
            
            if common_keywords:
                pattern_id = f"learned_pattern_{len(self.adaptive_patterns) + 1}_{datetime.now().strftime('%Y%m%d')}"
                
                new_pattern = AdaptivePattern(
                    pattern_id=pattern_id,
                    keywords=common_keywords,
                    cultural_context=[],  # ì¶”í›„ í™•ì¥
                    success_rate=0.7,  # ì´ˆê¸°ê°’
                    usage_count=0,
                    last_updated=datetime.now(),
                    examples=[ex.text for ex in scam_examples[:3]]
                )
                
                self.adaptive_patterns[pattern_id] = new_pattern
                
                logger.info(f"ğŸ†• ìƒˆ íŒ¨í„´ ë°œê²¬: {pattern_id} - í‚¤ì›Œë“œ: {common_keywords[:5]}")
    
    def _update_few_shot_pool(self):
        """Few-shot ì˜ˆì‹œ í’€ ì—…ë°ì´íŠ¸"""
        
        # ë†’ì€ ì‹ ë¢°ë„ì˜ ì˜¬ë°”ë¥¸ ì˜ˆì¸¡ ì‚¬ë¡€ë“¤ì„ Few-shot í’€ì— ì¶”ê°€
        high_quality_examples = [
            ex for ex in self.learning_examples 
            if ex.user_feedback == "correct" and ex.confidence > 0.8
        ]
        
        for example in high_quality_examples[:5]:  # ìµœëŒ€ 5ê°œë§Œ ì¶”ê°€
            category = self._categorize_example(example)
            
            if category and len(self.few_shot_pool[category]) < 10:  # ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ 10ê°œ
                new_example = {
                    "text": example.text,
                    "label": example.actual_label,
                    "reasoning": "ì‚¬ìš©ì ê²€ì¦ëœ ì‹¤ì œ ì‚¬ë¡€",
                    "confidence": example.confidence
                }
                
                self.few_shot_pool[category].append(new_example)
                logger.info(f"ğŸ“š Few-shot ì˜ˆì‹œ ì¶”ê°€: {category}")
    
    def _categorize_example(self, example: LearningExample) -> Optional[str]:
        """ì˜ˆì‹œë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
        
        text = example.text.lower()
        
        if any(keyword in text for keyword in ["ê¸ˆìœµê°ë…ì›", "ê²€ì°°", "ê²½ì°°", "êµ­ì„¸ì²­"]):
            return "government_scam"
        elif any(keyword in text for keyword in ["ì–´ë¨¸ë‹ˆ", "ì•„ë“¤", "ë”¸", "ì‚¬ê³ ", "ì‘ê¸‰ì‹¤"]):
            return "family_emergency" 
        elif example.actual_label == "legitimate":
            return "legitimate_calls"
        
        return None
    
    def _calculate_performance_metrics(self):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        
        if not self.learning_examples:
            return
        
        # ì •í™•ë„ ê³„ì‚°
        correct_predictions = sum(1 for ex in self.learning_examples if ex.user_feedback == "correct")
        total_predictions = len(self.learning_examples)
        
        current_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
        
        self.performance_tracker["accuracy_trend"].append({
            "timestamp": datetime.now().isoformat(),
            "accuracy": current_accuracy,
            "total_examples": total_predictions
        })
        
        # ìµœê·¼ 10ê°œ ê¸°ë¡ë§Œ ìœ ì§€
        if len(self.performance_tracker["accuracy_trend"]) > 10:
            self.performance_tracker["accuracy_trend"] = self.performance_tracker["accuracy_trend"][-10:]
        
        logger.info(f"ğŸ“Š í˜„ì¬ ì •í™•ë„: {current_accuracy:.3f} ({correct_predictions}/{total_predictions})")
    
    def _generate_analysis_id(self, text: str) -> str:
        """ë¶„ì„ ê³ ìœ  ID ìƒì„±"""
        return hashlib.md5(f"{text}_{datetime.now().isoformat()}".encode()).hexdigest()[:12]
    
    def _save_learning_data(self):
        """í•™ìŠµ ë°ì´í„° ì €ì¥"""
        
        try:
            self.config["data_persistence_path"].parent.mkdir(parents=True, exist_ok=True)
            
            save_data = {
                "timestamp": datetime.now().isoformat(),
                "adaptive_patterns": {
                    pid: asdict(pattern) for pid, pattern in self.adaptive_patterns.items()
                },
                "few_shot_pool": self.few_shot_pool,
                "performance_tracker": self.performance_tracker,
                "learning_examples_count": len(self.learning_examples)
            }
            
            with open(self.config["data_persistence_path"], 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"ğŸ’¾ í•™ìŠµ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {self.config['data_persistence_path']}")
            
        except Exception as e:
            logger.error(f"ğŸ’¾ í•™ìŠµ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _load_existing_data(self):
        """ê¸°ì¡´ í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
        
        try:
            if self.config["data_persistence_path"].exists():
                with open(self.config["data_persistence_path"], 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # ì ì‘í˜• íŒ¨í„´ ë³µì›
                for pid, pattern_data in data.get("adaptive_patterns", {}).items():
                    pattern_data["last_updated"] = datetime.fromisoformat(pattern_data["last_updated"])
                    self.adaptive_patterns[pid] = AdaptivePattern(**pattern_data)
                
                # Few-shot í’€ ì—…ë°ì´íŠ¸
                if "few_shot_pool" in data:
                    for category, examples in data["few_shot_pool"].items():
                        if category in self.few_shot_pool:
                            self.few_shot_pool[category].extend(examples)
                
                # ì„±ëŠ¥ ì¶”ì  ë°ì´í„° ë³µì›
                if "performance_tracker" in data:
                    self.performance_tracker.update(data["performance_tracker"])
                
                logger.info(f"ğŸ“š ê¸°ì¡´ í•™ìŠµ ë°ì´í„° ë¡œë“œ: íŒ¨í„´ {len(self.adaptive_patterns)}ê°œ")
                
        except Exception as e:
            logger.warning(f"ğŸ“š ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ (ì‹ ê·œ ì‹œì‘): {e}")
    
    def get_learning_status(self) -> Dict[str, Any]:
        """í•™ìŠµ ìƒíƒœ ì¡°íšŒ"""
        
        return {
            "total_analyses": self.performance_tracker["total_analyses"],
            "learning_examples": len(self.learning_examples),
            "adaptive_patterns": len(self.adaptive_patterns),
            "few_shot_pool_size": {
                category: len(examples) 
                for category, examples in self.few_shot_pool.items()
            },
            "learning_cycles": self.performance_tracker["learning_cycles"],
            "current_accuracy": self.performance_tracker["accuracy_trend"][-1]["accuracy"] if self.performance_tracker["accuracy_trend"] else 0,
            "ready_for_learning": len(self.learning_examples) >= self.config["learning_threshold"]
        }

# ê¸°ì¡´ analyzerë¥¼ í•™ìŠµ ê°•í™” ë²„ì „ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ
def create_learning_enhanced_analyzer():
    """í•™ìŠµ ê°•í™” ë¶„ì„ê¸° ìƒì„±"""
    return LearningEnhancedAnalyzer(llm_manager)

# ì‚¬ìš© ì˜ˆì œ
async def demo_learning_system():
    """í•™ìŠµ ì‹œìŠ¤í…œ ë°ëª¨"""
    
    analyzer = create_learning_enhanced_analyzer()
    
    # ë¶„ì„ ìˆ˜í–‰
    test_text = "ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ. ê¸ˆìœµê°ë…ì›ì—ì„œ ì—°ë½ë“œë ¸ìŠµë‹ˆë‹¤. ê³„ì¢Œ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤."
    result = await analyzer.analyze_with_learning(test_text)
    
    print("ğŸ” ë¶„ì„ ê²°ê³¼:")
    print(f"  ìœ„í—˜ë„: {result['final_risk_score']:.2f}")
    print(f"  ì‹ ë¢°ë„: {result['confidence']:.2f}")
    print(f"  Few-shot ì ìš©: {result['few_shot_applied']}")
    print(f"  íŒ¨í„´ ë§¤ì¹­: {result['patterns_matched']}ê°œ")
    
    # ì‚¬ìš©ì í”¼ë“œë°± ì‹œë®¬ë ˆì´ì…˜
    await analyzer.learn_from_feedback(
        analysis_id=result["analysis_id"],
        actual_label="scam",
        user_feedback="correct",
        user_id="demo_user"
    )
    
    # í•™ìŠµ ìƒíƒœ í™•ì¸
    status = analyzer.get_learning_status()
    print("\nğŸ“Š í•™ìŠµ ìƒíƒœ:")
    print(f"  ì´ ë¶„ì„: {status['total_analyses']}")
    print(f"  í•™ìŠµ ì˜ˆì‹œ: {status['learning_examples']}")
    print(f"  ì ì‘í˜• íŒ¨í„´: {status['adaptive_patterns']}")
    print(f"  í˜„ì¬ ì •í™•ë„: {status['current_accuracy']:.3f}")

if __name__ == "__main__":
    asyncio.run(demo_learning_system())